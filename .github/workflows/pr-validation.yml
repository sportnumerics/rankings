name: PR Validation

on:
  pull_request:
    types: [opened, synchronize, reopened]

permissions:
  id-token: write
  contents: read
  pull-requests: write

concurrency:
  group: pr-validation-${{ github.event.pull_request.number }}
  cancel-in-progress: true

jobs:
  # Detect which parts of the codebase changed
  detect-changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    outputs:
      backend: ${{ steps.filter.outputs.backend }}
      frontend: ${{ steps.filter.outputs.frontend }}
      infrastructure: ${{ steps.filter.outputs.infrastructure }}
      backtest: ${{ steps.filter.outputs.backtest }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            backend:
              - 'backend/**'
            frontend:
              - 'frontend/**'
            infrastructure:
              - 'infrastructure/**'
            backtest:
              - 'backend/lib/predict/**'
              - 'backend/scripts/backtest_baseline.py'
              - '.github/workflows/pr-validation.yml'

  # Run backend unit tests
  backend-unit-tests:
    name: Backend Unit Tests
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.backend == 'true'
    defaults:
      run:
        working-directory: backend
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      
      - name: Install uv
        run: pip install uv
      
      - name: Install dependencies
        run: uv sync
      
      - name: Run unit tests
        run: uv run python -m unittest discover -s lib -p "test_*.py" -t .

  # Run predictive backtest (only when model/backtest workflow changes)
  backend-backtest:
    name: Backend Backtest Baseline
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.backtest == 'true'
    defaults:
      run:
        working-directory: backend
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install uv
        run: pip install uv

      - name: Install dependencies
        run: uv sync

      - name: Restore backtest data cache
        uses: actions/cache@v4
        with:
          path: backend/.cache/backtest-data
          key: ${{ runner.os }}-backtest-data-v1

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: us-west-2
          role-to-assume: arn:aws:iam::265978616089:role/rankings-deployment-role-dev
          role-session-name: RankingsBacktestValidation

      - name: Sync backtest datasets from S3
        env:
          BACKTEST_BUCKET: sportnumerics-rankings-bucket-prod
          BACKTEST_YEARS: "2024 2025"
        run: |
          mkdir -p .cache/backtest-data
          for year in $BACKTEST_YEARS; do
            mkdir -p .cache/backtest-data/$year/games
            aws s3 cp s3://$BACKTEST_BUCKET/data/$year/games.json .cache/backtest-data/$year/games.json --only-show-errors
            aws s3 sync s3://$BACKTEST_BUCKET/data/$year/games .cache/backtest-data/$year/games --only-show-errors --size-only
          done

      - name: Run backtests
        env:
          BACKTEST_YEARS: "2024 2025"
        run: |
          mkdir -p out/backtest
          for year in $BACKTEST_YEARS; do
            uv run python -m scripts.backtest_baseline --data-dir .cache/backtest-data --year $year > out/backtest/$year.json
          done

      - name: Summarize backtest results
        run: |
          python - <<'PY'
          import json
          from pathlib import Path

          rows = []
          for p in sorted(Path('out/backtest').glob('*.json')):
              d = json.loads(p.read_text())
              y = d['year']
              t = d['team_backtest']
              rows.append((y, t['samples'], t['mae_margin'], t['winner_accuracy'], t['home_baseline_accuracy']))

          summary = ['## Backtest Baseline (team model)', '', '| Year | Samples | MAE Margin | Winner Acc | Home Baseline |', '|---|---:|---:|---:|---:|']
          for y, s, mae, acc, base in rows:
              summary.append(f"| {y} | {s} | {mae:.3f} | {acc:.3%} | {base:.3%} |")

          Path('out/backtest-summary.md').write_text('\n'.join(summary) + '\n')
          print('\n'.join(summary))
          PY

      - name: Upload backtest artifacts
        uses: actions/upload-artifact@v4
        with:
          name: backtest-results
          path: |
            backend/out/backtest/*.json
            backend/out/backtest-summary.md

  # Run frontend unit tests (if any exist)
  frontend-unit-tests:
    name: Frontend Unit Tests
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.frontend == 'true'
    defaults:
      run:
        working-directory: frontend
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json
      
      - name: Install dependencies
        run: npm ci
      
      - name: Run tests
        run: npm test --if-present

  # Deploy backend to dev environment
  deploy-backend:
    name: Deploy Backend to Dev
    runs-on: ubuntu-latest
    needs: [detect-changes, backend-unit-tests]
    if: needs.detect-changes.outputs.backend == 'true'
    environment: dev
    defaults:
      run:
        working-directory: backend
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: us-west-2
          role-to-assume: arn:aws:iam::265978616089:role/rankings-deployment-role-dev
          role-session-name: RankingsPRValidation
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.10.5"
      
      - name: Deploy backend
        run: ./deploy.sh dev

  # Run backend task to populate dev with limited data
  run-backend:
    name: Run Backend (Limited Scrape)
    runs-on: ubuntu-latest
    needs: [detect-changes, deploy-backend]
    if: needs.detect-changes.outputs.backend == 'true'
    environment: dev
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: us-west-2
          role-to-assume: arn:aws:iam::265978616089:role/rankings-deployment-role-dev
          role-session-name: RankingsPRValidation
      
      - name: Trigger backend ECS task with SCRAPE_LIMIT
        id: run-task
        run: |
          TASK_ARN=$(SCRAPE_LIMIT=10 ./backend/run.sh dev | jq -r '.tasks[0].taskArn')
          
          if [ -z "$TASK_ARN" ] || [ "$TASK_ARN" = "null" ]; then
            echo "Failed to start backend task."
            exit 1
          fi
          
          echo "task_arn=$TASK_ARN" >> "$GITHUB_OUTPUT"
          echo "Triggered task: $TASK_ARN"
      
      - name: Wait for backend task to complete
        run: |
          echo "Waiting for task ${{ steps.run-task.outputs.task_arn }} to complete..."
          TASK_ARN="${{ steps.run-task.outputs.task_arn }}"
          TASK_ID="${TASK_ARN##*/}"
          LOG_GROUP="/rankings/dev/backend"
          LOG_STREAM_PREFIX="rankings-backend/rankings-backend/${TASK_ID}"
          MAX_POLLS=60
          POLL_DELAY=10
          POLL=0
          LAST_TS=$(( $(date +%s) * 1000 ))

          print_new_logs() {
            EVENTS_JSON=$(aws logs filter-log-events \
              --log-group-name "$LOG_GROUP" \
              --log-stream-name-prefix "$LOG_STREAM_PREFIX" \
              --start-time "$LAST_TS" \
              --limit 200 \
              --output json 2>/dev/null || echo '{"events":[]}')

            EVENT_COUNT=$(echo "$EVENTS_JSON" | jq -r '.events | length // 0')
            if [ "$EVENT_COUNT" -gt 0 ]; then
              echo "$EVENTS_JSON" | jq -r '.events[] | "[task-log] \(.message)"'
              NEWEST_TS=$(echo "$EVENTS_JSON" | jq -r '[.events[].timestamp] | max')
              LAST_TS=$((NEWEST_TS + 1))
            fi
          }

          while true; do
            print_new_logs

            STATUS=$(aws ecs describe-tasks \
              --cluster rankings-backend-dev \
              --tasks "$TASK_ARN" \
              --query 'tasks[0].lastStatus' \
              --output text)

            echo "Current task status: $STATUS"

            if [ "$STATUS" = "STOPPED" ]; then
              print_new_logs
              break
            fi

            POLL=$((POLL + 1))
            if [ "$POLL" -ge "$MAX_POLLS" ]; then
              echo "Timed out waiting for backend task to stop."
              exit 1
            fi

            sleep "$POLL_DELAY"
          done
          
          EXIT_CODE=$(aws ecs describe-tasks \
            --cluster rankings-backend-dev \
            --tasks "$TASK_ARN" \
            --query 'tasks[0].containers[0].exitCode' \
            --output text)
          
          echo "Task exited with code: $EXIT_CODE"
          
          if [ "$EXIT_CODE" != "0" ]; then
            echo "Backend task failed!"
            exit 1
          fi

  # Deploy frontend to dev environment
  deploy-frontend:
    name: Deploy Frontend to Dev
    runs-on: ubuntu-latest
    needs: [detect-changes, frontend-unit-tests]
    if: needs.detect-changes.outputs.frontend == 'true'
    environment: dev
    defaults:
      run:
        working-directory: frontend
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json
      
      - name: Restore Next.js cache
        uses: actions/cache@v4
        with:
          path: frontend/.next/cache
          key: ${{ runner.os }}-nextjs-${{ hashFiles('frontend/package-lock.json') }}-${{ hashFiles('frontend/**/*.js', 'frontend/**/*.jsx', 'frontend/**/*.ts', 'frontend/**/*.tsx') }}
          restore-keys: |
            ${{ runner.os }}-nextjs-${{ hashFiles('frontend/package-lock.json') }}-
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: us-west-2
          role-to-assume: arn:aws:iam::265978616089:role/rankings-deployment-role-dev
          role-session-name: RankingsPRValidation
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.10.5"
      
      - name: Deploy frontend
        run: ./deploy.sh dev

  # Run E2E tests against dev environment
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    needs: [detect-changes, deploy-backend, run-backend, deploy-frontend]
    # Run if backend OR frontend changed (using always() to not skip if either deploy was skipped)
    if: |
      always() && 
      (needs.detect-changes.outputs.backend == 'true' || needs.detect-changes.outputs.frontend == 'true') &&
      (needs.deploy-backend.result == 'success' || needs.deploy-backend.result == 'skipped') &&
      (needs.run-backend.result == 'success' || needs.run-backend.result == 'skipped') &&
      (needs.deploy-frontend.result == 'success' || needs.deploy-frontend.result == 'skipped')
    defaults:
      run:
        working-directory: frontend
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json
      
      - name: Install dependencies
        run: npm ci
      
      - name: Install Playwright (Chromium)
        run: npx playwright install --with-deps chromium
      
      - name: Run E2E smoke tests
        env:
          BASE_URL: https://dev.sportnumerics.com
        run: npm run e2e
      
      - name: Upload Playwright report (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-report
          path: frontend/playwright-report/
          if-no-files-found: ignore

  # Post summary comment on PR
  post-summary:
    name: Post Summary
    runs-on: ubuntu-latest
    needs: [detect-changes, backend-unit-tests, backend-backtest, frontend-unit-tests, deploy-backend, run-backend, deploy-frontend, e2e-tests]
    if: always()
    steps:
      - name: Comment on PR
        uses: actions/github-script@v7
        with:
          script: |
            const backendChanged = '${{ needs.detect-changes.outputs.backend }}' === 'true';
            const frontendChanged = '${{ needs.detect-changes.outputs.frontend }}' === 'true';
            const backtestChanged = '${{ needs.detect-changes.outputs.backtest }}' === 'true';
            
            const backendTests = '${{ needs.backend-unit-tests.result }}';
            const backendBacktest = '${{ needs.backend-backtest.result }}';
            const frontendTests = '${{ needs.frontend-unit-tests.result }}';
            const backendDeploy = '${{ needs.deploy-backend.result }}';
            const runBackend = '${{ needs.run-backend.result }}';
            const frontendDeploy = '${{ needs.deploy-frontend.result }}';
            const e2eTests = '${{ needs.e2e-tests.result }}';
            
            const statusIcon = (result) => {
              if (result === 'success') return '‚úÖ';
              if (result === 'failure') return '‚ùå';
              if (result === 'skipped') return '‚è≠Ô∏è';
              return '‚ö†Ô∏è';
            };
            
            let body = '## PR Validation Results\n\n';
            
            if (backendChanged) {
              body += '### Backend\n';
              body += `- ${statusIcon(backendTests)} Unit Tests\n`;
              if (backtestChanged) {
                body += `- ${statusIcon(backendBacktest)} Backtest Baseline (2024/2025)\n`;
              }
              body += `- ${statusIcon(backendDeploy)} Deploy to Dev\n`;
              body += `- ${statusIcon(runBackend)} Run Backend (scrape 10 teams/div + predict + sync)\n\n`;
            }
            
            if (frontendChanged) {
              body += '### Frontend\n';
              body += `- ${statusIcon(frontendTests)} Unit Tests\n`;
              body += `- ${statusIcon(frontendDeploy)} Deploy to Dev\n\n`;
            }
            
            if (backendChanged || frontendChanged) {
              body += `### E2E Tests\n`;
              body += `- ${statusIcon(e2eTests)} Smoke Tests\n\n`;
            }
            
            if (!backendChanged && !frontendChanged) {
              body += '_No backend or frontend changes detected._\n\n';
            }
            
            const allSuccess = 
              (!backendChanged || (
                backendTests === 'success' &&
                (!backtestChanged || backendBacktest === 'success') &&
                backendDeploy === 'success' &&
                runBackend === 'success'
              )) &&
              (!frontendChanged || (frontendTests === 'success' && frontendDeploy === 'success')) &&
              (!(backendChanged || frontendChanged) || e2eTests === 'success');
            
            if (allSuccess && (backendChanged || frontendChanged)) {
              body += '\nüöÄ **All checks passed!** Preview deployed to https://dev.sportnumerics.com\n\nMerge to deploy to production.';
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
